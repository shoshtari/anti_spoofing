{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cachetools\n",
    "import random \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@cachetools.cached(cache={})\n",
    "def read_video(path: str, frame_index: int = None, min_brightness: int = 10):\n",
    "    video = cv2.VideoCapture(path)\n",
    "    if not video.isOpened():\n",
    "        raise IOError(f\"Error opening video file: {path}\")\n",
    "    \n",
    "\n",
    "    \n",
    "    frames = []\n",
    "    def normalize(img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "        img = cv2.normalize(img, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_8UC1)\n",
    "        img = cv2.resize(img, (480, 640), None)\n",
    "        return img \n",
    "    \n",
    "    while video.isOpened():\n",
    "        ret, frame = video.read()\n",
    "        frames.append(frame)\n",
    "        if not ret:\n",
    "            break\n",
    "        if len(frames) >= frame_index and np.average(frame) > min_brightness:\n",
    "            return normalize(frame)\n",
    "    if frame_index is None:\n",
    "        frame_index = len(frames) // 2\n",
    "        while np.average(frames[frame_index]) < min_brightness:\n",
    "            frame_index += 1\n",
    "        return normalize(frames[frame_index]) \n",
    "    raise ValueError(\"frame not found\")\n",
    "REAL_FILES = (\n",
    "    \"1.avi\",\n",
    "    \"2.avi\",\n",
    "    \"HR_1.avi\",\n",
    "    \"HR_4.avi\",\n",
    ")\n",
    "def read_dir(path: str):\n",
    "    real_images = []\n",
    "    spoof_images = []\n",
    "    for file in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, file)):\n",
    "            r, s = read_dir(os.path.join(path, file))\n",
    "            real_images.extend(r)\n",
    "            spoof_images.extend(s)\n",
    "        else:\n",
    "            if file.endswith(\".avi\"):\n",
    "                obj = (os.path.join(path, file))\n",
    "                if file in REAL_FILES:\n",
    "                    real_images.append(obj)\n",
    "                    \n",
    "                else:\n",
    "                    spoof_images.append(obj)\n",
    "    return real_images, spoof_images\n",
    "\n",
    "\n",
    "def show_data(ds, rows, cols):\n",
    "    random.shuffle(ds)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ind = i * cols + j \n",
    "            plt.subplot(rows, cols, ind + 1)\n",
    "            plt.imshow(ds[ind])\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "\n",
    "def load_data(path: str, show = False):\n",
    "    @cachetools.cached(cache={})\n",
    "    def inner(path):\n",
    "        r, s = read_dir(path)\n",
    "        real = [read_video(i, frame_index=1) for i in r]\n",
    "        spoof = [read_video(i, frame_index=1) for i in s]\n",
    "        return real, spoof\n",
    "    real, spoof = inner(path)\n",
    "    if show:\n",
    "        show_data(spoof, 4, 3)\n",
    "        plt.figure()\n",
    "        show_data(real, 4, 3)\n",
    "    ds = [(i, 0) for i in real] + [(i, 1) for i in spoof]\n",
    "    random.shuffle(ds)\n",
    "    x = np.array([i[0] for i in ds])\n",
    "    y = np.array([i[1] for i in ds])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_data(\"./datasets/train_release\")\n",
    "x_test, y_test = load_data(\"./datasets/test_release\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input must have 3 channels; Received `input_shape=(640, 480, 2)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Deep_Models_CVPR_2018_paper.pdf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mobilenet \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplications\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMobileNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m mobilenet\u001b[38;5;241m.\u001b[39mlayers[:\u001b[38;5;28mlen\u001b[39m(mobilenet\u001b[38;5;241m.\u001b[39mlayers)]:\n\u001b[1;32m     13\u001b[0m     l\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/w/uni/CV/venv/lib/python3.12/site-packages/keras/src/applications/mobilenet.py:140\u001b[0m, in \u001b[0;36mMobileNet\u001b[0;34m(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes, classifier_activation)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m         default_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m224\u001b[39m\n\u001b[0;32m--> 140\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m \u001b[43mimagenet_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobtain_input_shape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_data_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequire_flatten\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mimage_data_format() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchannels_last\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    150\u001b[0m     row_axis, col_axis \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/w/uni/CV/venv/lib/python3.12/site-packages/keras/src/applications/imagenet_utils.py:383\u001b[0m, in \u001b[0;36mobtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`input_shape` must be a tuple of three integers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    384\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input must have 3 channels; Received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`input_shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m     )\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    388\u001b[0m     input_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m input_shape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m min_size\n\u001b[1;32m    389\u001b[0m ) \u001b[38;5;129;01mor\u001b[39;00m (input_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m input_shape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m min_size):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput size must be at least \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The input must have 3 channels; Received `input_shape=(640, 480, 2)`"
     ]
    }
   ],
   "source": [
    "# https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Learning_Deep_Models_CVPR_2018_paper.pdf\n",
    "mobilenet = tf.keras.applications.MobileNet(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(640, 480, 3),\n",
    "    \n",
    "    pooling=None,\n",
    "    classes=2,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "for l in mobilenet.layers[:len(mobilenet.layers)]:\n",
    "    l.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(mobilenet)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dropout(rate = 0.8))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fourier(img) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # img = cv2.resize(img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    fourier = cv2.dft(np.float32(gray), flags=cv2.DFT_COMPLEX_OUTPUT)\n",
    "    \n",
    "    fourier_shift = np.fft.fftshift(fourier)\n",
    "\n",
    "    magnitude = 20*np.log(cv2.magnitude(fourier_shift[:,:,0],fourier_shift[:,:,1]))\n",
    "    magnitude = cv2.normalize(magnitude, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_8UC1)\n",
    " \n",
    "    phase = cv2.phase(fourier_shift[:,:,0],fourier_shift[:,:,1])\n",
    "    phase = cv2.normalize(phase, None, 0, 1, cv2.NORM_MINMAX, cv2.CV_8UC1)\n",
    "\n",
    "\n",
    "    \n",
    "    return cv2.merge(magnitude, phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salt_pepper(img, rate = 0.2): \n",
    "    \n",
    "    w, h, _ = img.shape \n",
    "    noised_image = img.copy()\n",
    "    white = (255, 255, 255)\n",
    "    black = (0, 0, 0)\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            if random.random() > rate:\n",
    "                continue\n",
    "            noised_image[i, j] = white if random.random() > 0.5 else black \n",
    "    return noised_image\n",
    "\n",
    "noised_image = [add_salt_pepper(i) for i in x_train]\n",
    "blurred_image = [cv2.GaussianBlur(i, (13, 13), 10) for i in x_train]\n",
    "\n",
    "augmented_x = np.concatenate([x_train, noised_image, blurred_image])\n",
    "augmented_y = np.concatenate([y_train, y_train, y_train])\n",
    "ind = list(range(len(augmented_x)))\n",
    "random.shuffle(ind)\n",
    "x_train_final = [None for _ in range(len(augmented_x))]\n",
    "y_train_final = [None for _ in range(len(augmented_x))]\n",
    "for i in range(len(ind)):\n",
    "    x_train_final[i] = augmented_x[ind[i]]\n",
    "    y_train_final[i] = augmented_y[ind[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 640, 480, 3) (720, 640, 480, 3)\n",
      "(240,) (720,)\n",
      "(360, 640, 480, 3) (360,)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape, y_train_final\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_test\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mget_fourier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(get_fourier(x_test),y_test), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(len(x_train_final), len(y_train_final), len(x_train))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m, in \u001b[0;36mget_fourier\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_fourier\u001b[39m(img) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# img = cv2.resize(img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     fourier \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mdft(np\u001b[38;5;241m.\u001b[39mfloat32(gray), flags\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mDFT_COMPLEX_OUTPUT)\n\u001b[1;32m      7\u001b[0m     fourier_shift \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfft\u001b[38;5;241m.\u001b[39mfftshift(fourier)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train_final = np.array(x_train_final)\n",
    "y_train_final = np.array(y_train_final)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "print(x_train.shape, x_train_final.shape)\n",
    "print(y_train.shape, y_train_final.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "model.fit(get_fourier(x_train), y_train, epochs=20, validation_data=(get_fourier(x_test),y_test), batch_size=12)\n",
    "\n",
    "\n",
    "# print(len(x_train_final), len(y_train_final), len(x_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
