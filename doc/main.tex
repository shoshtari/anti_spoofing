
\documentclass{article}

\usepackage[a4paper, margin=0.5in,]{geometry}

\usepackage{graphicx}

\usepackage{anyfontsize}
\usepackage{wrapfig}
\usepackage{amsmath} % xrightarrow
\usepackage{xepersian}
\settextfont[Scale=1.1]  {Vazirmatn-Medium}
\linespread{1}
\newcommand{\integral}[2]{\int\limits_{#1}^{#2}}
\newcommand{\Lim}{\dispxlaystyle\lim}
\newcommand{\AnswerSpace}{~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\}
\begin{document}
	\noindent
	\textbf{تحلیل و طراحی الگوریتم ها} 
	\textbf{
	پروژه درس بینایی ماشین
	}
	\\ \\ 
	
	
	\begin{minipage}{0.9\textwidth}
		~\\ ~\\
		 مرتضی ملکی نژاد - محمدجواد جلیلوند \\
		 ۴۰۰۵۲۲۲۱۱ - ۴۰۰۵۲۱۲۱۶
	\end{minipage}
	\noindent\begin{minipage}{0.1\textwidth}% adapt widths of minipages to your needs
		\begin{flushleft}
			
			\includegraphics[width=\linewidth]{logo.png}
		\end{flushleft}
	\end{minipage}%
	\\ \\
	\hrule ~\\ ~\\
	
	\section{استخراج ویژگی}
	برای این بخش، ابتدا ما به راه های معمول مثل LBP و استفاده از هیستوگرام فکر کردیم و در ابتدا یک classifier بر روی هیستوگرام LBP تصاویر پیاده سازی کردیم که دقتی در حدود ۰.۶۶ میداد.
	
	در گام بعدی ما سعی کردیم کمی مراحل پیش پردازش را سرعت دهیم تا بتوانیم به راحتی یک مدل را در کانفیگ های مختلف تست بگیریم، برای این کار تابع feature extraction را با استفاده از multiprocessing به طور موازی پیاده کردیم.
	
	سپس سعی کردیم که دقت مدل را بهبود دهیم، اولین قدم ما افزایش ویژگی هایی بود که classifier بر پایه آن ها تصمیم گیری میکرد، پس فوریه و هیستوگرام رنگی تصویر را نیز اضافه کردیم ولی برای هیستوگرام رنگی به جای اینکه در فضای RGB هیستوگرام را حساب کنیم،‌ ابتدا تصویر را به فضای HSV بردیم. زیرا به نظرمان به طور خاص saturation میتوانست کمکمان کند. زیرا برخلاف تصویر زنده که درخشش نور روی جاهای خاصی همانند چشم، گونه و یا پیشانی است،‌در LCD درخشش نور به طور بازتاب یک منبع نوری خارجی مثل چراغ، خورشید و یا فلش دوربین است و saturation در یک ناحیه دایره ای پیک میزند. استفاده از saturation باعث شد که مدل ما دچار بیش برازش هم نشود و به دقت هشتاد درصد برسیم.
	
	مرحله بعدی کار ما tune کردن classifier بود. ما برای classify کردن تصاویر از یک SVM با کرنل rbf بود که متغیر های آن را هیچ تغییری نداده بودیم.
	اول کمی سعی کردیم C را تنظیم بکنیم تا موارد اشتباه را کمتر کنیم،‌ سپس سعی کردیم ایده ای مشابه random forest را روی svm پیاده سازی کنیم. به اینصورت که به جای آموزش یک SVM با هزار داده، ۵ svm با ۲۰۰ داده آموزش دهیم و بین آن ها رای بگیریم، ولی متاسفانه این موضوع دقت ما را افزایش نداد.
	
	مراحلی که میخواستیم سراغ آن برویم ولی وقت نشد از قرار زیر است:
	\begin{itemize}
		\item استفاده از ابزاری هایی مثل hough line detection برای تشخیص مستطیل
		\item اضافه کردن SIFT به ویژگی های استخراج شده.
		\item استفاده از data augmentaion که بتوانیم نا همسانی داده را کنترل کنیم
	\end{itemize}
	\subsection{فوریه}
	برای قسمت فوریه، ما به جای اینکه از تصویر خاکستری فوریه بگیریم، از کانال s تصویر فوریه گرفتیم، دلیل این امر این بود که همانطور که گفتیم تغییرات s برای ما به مراتب مهم تر از تغییرات سایر چنل ها (چه RGB و چه HSV ) است. با این کار توانستیم به دقت ۸۴ درصد برسیم. 
	کار دیگری که برای این موضوع کردیم،‌این بود که سعی کردیم عناصر فرکانس پایین را حذف کنیم تا تمرکز classifier بیشتر به سمت جزئیات برود ولی این موضوع خیلی تاثیری نداشت.
	یکی دیگر از کارهای ما این بود که به جای خود فوریه، از log آن استفاده کردیم تا اختلاف بین اعضا زیاد نشود،‌زیرا در شبکه های عصبی، بهتر است که داده به صورت نرمال بیاید ولی ما چون مقدار بیشینه را نداشتیم نمیتوانستیم به صورت کامل داده ها را نرمالیزه کنیم پس به گرفتن لوگاریتم اکتفا کردیم.
	
	\section{یادگیری عمیق}
	برای این بخش، ما پایه مدل را  مدل از پیش آموخته mobilenet با وزن های imagenet گرفتیم، زیرا پارامتر های آن به نسبت شبکه های دیگر همچون VGG، YOLO و ... کمتر بود (هرچند بعدا سعی کردیم VGG را هم امتحان کنیم) ما همه لایه های mobilenet را فریز کردیم و لایه نهایی آن را حذف کردیم و به جای آن یک لایه FC برای تصمیم گیری نهایی با تابع فعال ساز softmax در نظر گرفتیم،‌ و برای شروع ۲۰ epoch برای آموزش در نظر گرفتیم ولی مشاهده کردیم که تقریبا پس از ۷ epoch مدل اشباع شد و به دقت ۱۰۰ رسید ولی وقتی رو داده های تست مدل را ران کردیم،‌دقت ۹۰ گرفتیم و دیدیم overfit رخ داده،‌برای این کار یکی از اولین کارهایی که کردیم اضافه کردن یک لایه dropout قبل از لایه FC بود. سپس سعی کردیم که با تغییر عکس ها مسئله را برای classifier سخت تر کنیم. به این منظور نویز نمک و فلفل را به تصویر اضافه کردیم،، با فیلتر گاوسی مقداری تصویر را تار کردیم.
	
\end{document}
